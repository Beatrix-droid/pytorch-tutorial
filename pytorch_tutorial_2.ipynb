{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkb4wbZwvmWHEFSBJMszaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beatrix-droid/pytorch-tutorial/blob/master/pytorch_tutorial_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "g92CDjjRqU_m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Learning Pytorch\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "P8a5TFKLqWL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a tensor"
      ],
      "metadata": {
        "id": "rGhh2mcZwC6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a tensor:\n",
        "import torch\n",
        "\n",
        "x = torch.empty(2,2) # a 2d tensor\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfaHP0_brG9x",
        "outputId": "5182016d-890b-4063-ba7c-3099d48e8173"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0423e-36, 0.0000e+00],\n",
            "        [3.3631e-44, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "can sue teh rand ne==method to create a tensor with random numbers"
      ],
      "metadata": {
        "id": "I03pJgRLryX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#can create tensors from lists as well:\n",
        "torch.tensor([[1., -1.], [1., -1.]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGhuMhg-sJIi",
        "outputId": "1790fd52-111b-4364-e929-26f75e775d7c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1., -1.],\n",
              "        [ 1., -1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#can add tensors as you would normal add ints or concat strings\n",
        "\n",
        "#can also add one tensor to another:\n",
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "y.add_(x) #add tensor x to y. By defualt any function with a trailing underscore in pytorch will be an inplace operation\n",
        "\n",
        "\n",
        "#can also divide and multiplu as one normally woukld"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP8Z-L9ZsRD9",
        "outputId": "dd39ec67-279c-4d05-f6fc-5e154282c401"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1679, 1.1410],\n",
              "        [1.8046, 1.2823]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting a tensor to a numpy array"
      ],
      "metadata": {
        "id": "LotxgwlNwJs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a=torch.ones(5) #a 1d tensor of len 5 full of ones\n",
        "b = a.numpy() #conver to array\n",
        "print(a)\n",
        "print(type(b))\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgpZNMjRwOm_",
        "outputId": "9f4e3f03-e574-40a7-eeb3-fdf02c827846"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "<class 'numpy.ndarray'>\n",
            "[1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "converting an array to a tensor"
      ],
      "metadata": {
        "id": "ry9L9Zcxxg54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a= np.ones(5) #instantiate the array\n",
        "b=torch.from_numpy(a) #convert the arary to a pytorch tensor\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IocdHmXwk91",
        "outputId": "3e22113f-adf5-43fb-f9ff-e1bf39a782ba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check if you have a cuda toolkit avaialble to do operations on the gpu"
      ],
      "metadata": {
        "id": "FHEe8ZCPyJo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  x= torch.ones(5, device=device)\n",
        "  y=torch.ones(5)\n",
        "  y=y.to(device) #create and move tensors to gpu\n",
        "#if you now try to use numpy n calling  \n",
        "  x.numpy()\n",
        "  #you will get an error because numpy can only handle \n",
        "  #cpu tensors so we would have to move it back to the cpu\n",
        "  x=x.to('cpu')\n",
        "  "
      ],
      "metadata": {
        "id": "W8FYCCUvx01w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "grad = true\n",
        "when creatinga  tensor in pytorch we can set an optional parameter to True: requires_grad=True  This tells pytorch that it will have to calculate teh gradient of the tensor later on in the computation"
      ],
      "metadata": {
        "id": "WUV-0rbK8BJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5, requires_grad=True)"
      ],
      "metadata": {
        "id": "ADC_dx748AZF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd Pytorch\n",
        "calculate gradients to optimize models"
      ],
      "metadata": {
        "id": "nm_zmOaX8kXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m-mynKU8eIt",
        "outputId": "96c2f9c1-ed24-4bd2-b87a-e6a02a860abc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6061, 0.6769, 0.1188])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#say we need ot calculate the gradient of a functionat that point.\n",
        "#we set requires_grad=True:\n",
        "x = torch.rand(3, requires_grad=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "b3O_4uRN80k-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now do an operation:\n",
        "y = x+2\n",
        "print(y) #operation was addition so the grad_funct was add\n",
        "z = y*y*2\n",
        "print(z)#operation was mult so the grad_funct was mult\n",
        "z=z.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x_Gxm149QTr",
        "outputId": "c44b33be-8875-4c54-e6eb-2933cc916c03"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.9680, 2.8814, 2.8509], grad_fn=<AddBackward0>)\n",
            "tensor([17.6181, 16.6053, 16.2553], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now when we want to calculate the gradients the only thing that we need to do is call the .backward() method:"
      ],
      "metadata": {
        "id": "Au3fCgqo_b6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()# will calculate the gradient of z with respect to x so dz/dx"
      ],
      "metadata": {
        "id": "W6OtYGtN_acZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now x has a grad attribute with teh gradients stored that we can use:\n",
        "\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aObXBxBM_v8_",
        "outputId": "d6d9aeae-8e1a-47c2-fddc-0a8255ba2d02"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3.9573, 3.8419, 3.8012])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case z was a scalar (as z =z.mean()) so we could just implciitly call the backward function on it.\n",
        "\n",
        "the backward function is based on the chain rule  (jacobain matrix)(vector) = (gradients we are interested in\n",
        "![image info](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "914eY5SIAxq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "in general if z is a vector you need to pass the value f the vector in the function.Say we had something like:\n",
        "```\n",
        "v= torch.tensor([0.1. 1.0, 0.001],dtype=torch.float32)\n",
        "z = y*y*2 #z is now a bector\n",
        "z.backward(v)\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9iJV-1EjAxw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#preventing Pytorch from tracking the history\n",
        " (useful when updating weights of a model)\n",
        "\n",
        " We have three options on how to do this:\n",
        " \n",
        " ```\n",
        " 1) x.requres_grad_(False)\n",
        " 2) x.detach()\n",
        " 3) with torch.no_grad():\n",
        " ```"
      ],
      "metadata": {
        "id": "ukH0fUSADq5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#method 1\n",
        "x.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsV4YrHpDqFf",
        "outputId": "476f6306-1f43-4800-fa7f-5b4e4d263a27"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9680, 0.8814, 0.8509])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method 2\n",
        "y = x.detach() #creates a new tensor with teh same values but no gradient\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQocqZlWEZyf",
        "outputId": "c82b4c86-8043-4bce-e1da-ff2e04216db2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9680, 0.8814, 0.8509])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method 3\n",
        "with torch.no_grad():\n",
        "  y = x+2\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EckgFxwGEjYH",
        "outputId": "cd064119-80ff-42c5-c36c-e37248cfa23f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.9680, 2.8814, 2.8509])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "warning, whenever we call the backward  function then the gradient for the tensors will be accumulated and summed up in the .grad attribute:"
      ],
      "metadata": {
        "id": "9Rywk4QOE3vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "for epoch in range(2):\n",
        "  model_output= (weights*3).sum()\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK9H59LPEyl_",
        "outputId": "70413226-0206-4560-9052-3e025811339b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for the second one the gradients are incorrect. they have neen usmmed up hence 6\n",
        "\n",
        "#must clear the weights each time:\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "for epoch in range(2):\n",
        "  model_output= (weights*3).sum()\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)\n",
        "  weights.grad.zero_()#this is the important line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZXELvTMFsKz",
        "outputId": "012429c8-d23e-4811-f774-48e474e9d118"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent and constructing a basic pipeline for pytorch\n",
        "let's see how to optimize paarameters from scracth by using a linear regression model that we will code from scratch"
      ],
      "metadata": {
        "id": "oCKBegWqvmJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w+x\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X=np.array([1,2,3,4], dtype=np.float32)\n",
        "Y=np.array([2,4,6,8], dtype=np.float32)\n",
        "\n",
        "w=0.0"
      ],
      "metadata": {
        "id": "QMNkbEOCvtIi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "\n",
        "#least square residual loss function\n",
        "def loss(y, y_predicted):\n",
        "  return((y_predicted -y)**2).mean()\n",
        "\n",
        "#gradient\n",
        "#MSE = 1/n *(wx-y)^2 formual for mean squared error\n",
        "#dj/dw = 1/N 2x(wx-y)\n",
        "#implement this fromulka in the gradient: y_predicted= y_predicted\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean() #mean as dividing by n in formula\n"
      ],
      "metadata": {
        "id": "IArJMoQK11q7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"prediction before training: f(5)={forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAS6FC1T18KA",
        "outputId": "995d758c-f007-4677-8eb9-1622b2759320"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5)=0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters=10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  \n",
        "  #prediction\n",
        "  y_pred = forward(X)\n",
        "  \n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradient\n",
        "  dw=gradient(X,Y, y_pred)\n",
        "\n",
        "  #update weights\n",
        "  w=w-learning_rate *dw\n",
        "\n",
        "  if epoch % 1 ==0:\n",
        "    print(f\"epoch {epoch+1}: W={w}, loss = {l}\")\n",
        "  print(f\"prediction after trainingf(5)={forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ZNcURJ2L_b",
        "outputId": "25304510-502a-4826-9ac5-ce7c05ca230f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: W=1.2, loss = 30.0\n",
            "prediction after trainingf(5)=6.0\n",
            "epoch 2: W=1.6799999618530272, loss = 4.799999237060547\n",
            "prediction after trainingf(5)=8.399999809265136\n",
            "epoch 3: W=1.871999988555908, loss = 0.7680001854896545\n",
            "prediction after trainingf(5)=9.35999994277954\n",
            "epoch 4: W=1.9487999868392942, loss = 0.1228799968957901\n",
            "prediction after trainingf(5)=9.743999934196472\n",
            "epoch 5: W=1.9795200133323667, loss = 0.019660834223031998\n",
            "prediction after trainingf(5)=9.897600066661834\n",
            "epoch 6: W=1.9918080282211301, loss = 0.0031457357108592987\n",
            "prediction after trainingf(5)=9.95904014110565\n",
            "epoch 7: W=1.9967231869697568, loss = 0.0005033080233260989\n",
            "prediction after trainingf(5)=9.983615934848784\n",
            "epoch 8: W=1.99868928194046, loss = 8.053186320466921e-05\n",
            "prediction after trainingf(5)=9.993446409702301\n",
            "epoch 9: W=1.999475698471069, loss = 1.2884394891443662e-05\n",
            "prediction after trainingf(5)=9.997378492355345\n",
            "epoch 10: W=1.999790253639221, loss = 2.0613531432900345e-06\n",
            "prediction after trainingf(5)=9.998951268196105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we did the computation of weights manually\n",
        "now lets do the same thing with pytorch. No need for numpy arrays, will use Pytorch tensors"
      ],
      "metadata": {
        "id": "770Ff1_y5QWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "X=torch.tensor([1,2,3,4], dtype=torch.float32, requires_grad=True)\n",
        "Y=torch.tensor([2,4,6,8], dtype=torch.float32,  requires_grad=True)\n",
        "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"
      ],
      "metadata": {
        "id": "donc5Z5_5MjF"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "\n",
        "#least square residual loss function\n",
        "def loss(y, y_predicted):\n",
        "  return((y_predicted -y)**2).mean()\n",
        "\n",
        "#gradient\n",
        "#MSE = 1/n *(wx-y)^2 formual for mean squared error\n",
        "#dj/dw = 1/N 2x(wx-y)\n",
        "#implement this fromulka in the gradient: y_predicted= y_predicted\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean() #mean as dividing by n in formula"
      ],
      "metadata": {
        "id": "cwLWO-sG5rkV"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_iters):\n",
        "  \n",
        "  #prediction\n",
        "  y_pred = forward(X)\n",
        "  \n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradient\n",
        "  l.backward() #dl/dw\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    w-= learning_rate * w.grad\n",
        "\n",
        "  w.grad.zero_()  \n",
        "  if epoch % 1 ==0:\n",
        "    print(f\"epoch {epoch+1}: W={w}, loss = {l}\")\n",
        "  print(f\"prediction after trainingf(5)={forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "YaL72pHr9Ru4",
        "outputId": "dfc1fa67-2f5e-410e-91ec-655e60af332b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-3dece198f54a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m#update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "loss=nn.MSELoss()\n",
        "optimizer=torch.optim.SGD([w], lr=learning_rate)"
      ],
      "metadata": {
        "id": "v-5MOpcz_RtY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "replacing the optimizers and the loss function"
      ],
      "metadata": {
        "id": "na7FL5fwKUTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_iters):\n",
        "  \n",
        "  #prediction\n",
        "  y_pred = forward(X)\n",
        "  \n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradient\n",
        "  l.backward() #dl/dw\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if epoch % 1 ==0:\n",
        "    print(f\"epoch {epoch+1}: W={w}, loss = {l}\")\n",
        "  print(f\"prediction after trainingf(5)={forward(5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X55OaDyUChcw",
        "outputId": "6a83981c-924f-411d-eaa8-09001fca1467"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 2: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 3: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 4: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 5: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 6: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 7: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 8: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 9: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n",
            "epoch 10: W=0.29999998211860657, loss = 21.674999237060547\n",
            "prediction after trainingf(5)=1.4999998807907104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "replacing the forward function"
      ],
      "metadata": {
        "id": "DrIcyLBqKXoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is where we would define the model\n",
        "X=torch.tensor([[1],[2],[3],[4]], dtype=torch.float32, requires_grad=True)\n",
        "Y=torch.tensor([[2],[4],[6],[8]], dtype=torch.float32,  requires_grad=True)\n",
        "X_test= torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "imput_size = n_features\n",
        "output_size = n_features\n",
        "model = nn.Linear(imput_size, output_size)"
      ],
      "metadata": {
        "id": "0Exh-639KMrY"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=nn.MSELoss()\n",
        "#updates the weights\n",
        "optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "PBarsAP9LZZ1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_iters):\n",
        "  \n",
        "  #prediction\n",
        "  y_pred = forward(X)\n",
        "  \n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradient\n",
        "  l.backward() #dl/dw\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if epoch % 1 ==0:\n",
        "    [w,b]=model.parameters()\n",
        "    print(f\"epoch {epoch+1}: W={w}, loss = {l}\")\n",
        "  print(f\"prediction after trainingf(5)={model(X_test).item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjVn3fNuLur0",
        "outputId": "48ea6a86-d527-4e25-8ad1-9b036ceff66e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 1.7469966411590576\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 2: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 3: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 4: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 5: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 6: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 7: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 8: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 9: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n",
            "epoch 10: W=Parameter containing:\n",
            "tensor([[-0.0340]], requires_grad=True), loss = 31.02906036376953\n",
            "prediction after trainingf(5)=0.4507153034210205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "template for creating our custom linear regression model:\n"
      ],
      "metadata": {
        "id": "ixis9skNMTC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  \n",
        "  def __init__(self, imput_dim, ouptut_dim):\n",
        "    super(LinearRegression,self).__init__()\n",
        "    #define layers:\n",
        "    self.lin= nn.Linear(imput_dim, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "YXjEDb5KMShr"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiating an instance of the model:\n",
        "model=LinearRegression(imput_size,output_size)\n",
        "print(f\"Prediction before training f(5)={model(X_test).item()}\")\n",
        "for epoch in range(n_iters):\n",
        "  \n",
        "  #prediction\n",
        "  y_pred = forward(X)\n",
        "  \n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  #gradient\n",
        "  l.backward() #dl/dw\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if epoch % 1 ==0:\n",
        "    [w,b]=model.parameters()\n",
        "    print(f\"epoch {epoch+1}: W={w}, loss = {l}\")\n",
        "  print(f\"prediction after trainingf(5)={model(X_test).item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAMJ-KztNEqr",
        "outputId": "10d8aff8-878b-44fd-a599-b088c3ead5d8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training f(5)=-3.090867519378662\n",
            "epoch 1: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 46.72861099243164\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 2: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 3: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 4: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 5: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 6: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 7: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 8: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 9: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n",
            "epoch 10: W=Parameter containing:\n",
            "tensor([[-0.5877]], requires_grad=True), loss = 50.223365783691406\n",
            "prediction after trainingf(5)=-3.090867519378662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression in Pytorch"
      ],
      "metadata": {
        "id": "6erKBKjXOa83"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZ-6K_IFOGXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}